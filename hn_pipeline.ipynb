{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hacker News Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll work with a sequence of basic natural language processing tasks using a created pipeline class. Our goal is to understand the tech topics in 2014 by finding the top 100 keywords of Hacker News posts in 2014. \n",
    "\n",
    "Our input data is from Hacker News (HN), a website about computer science and entrepreneurship posts that community vote stories,  API that returns JSON data of the top stories in 2014. The list of JSON posts is in 'hn_stories_2014.json'.\n",
    "\n",
    "We will deal with the following keys of the posts:\n",
    "\n",
    "* created_at: A timestamp of the story's creation time.\n",
    "* created_at_i: A unix epoch timestamp.\n",
    "* url: The URL of the story link.\n",
    "* objectID: The ID of the story.\n",
    "* author: The story's author (username on HN).\n",
    "* points: The number of upvotes the story had.\n",
    "* title: The headline of the post.\n",
    "* num_comments: The number of a comments a post has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base of this project is the class below created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "# DAG class to deal with graph\n",
    "class DAG():\n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "        \n",
    "    def in_degrees(self):\n",
    "        in_degrees = {}\n",
    "        for node in self.graph:\n",
    "            if node not in in_degrees:\n",
    "                in_degrees[node] = 0\n",
    "            for pointed in self.graph[node]:\n",
    "                if pointed not in in_degrees:\n",
    "                    in_degrees[pointed] = 0\n",
    "                in_degrees[pointed] += 1\n",
    "        return in_degrees\n",
    "    \n",
    "    def sort(self):\n",
    "        in_degrees = self.in_degrees()\n",
    "        to_visit = deque()\n",
    "        for node in self.graph:\n",
    "            if in_degrees[node] == 0:\n",
    "                to_visit.append(node)\n",
    "                \n",
    "        searched = []\n",
    "        while to_visit:\n",
    "            node = to_visit.popleft()\n",
    "            for pointer in self.graph[node]:\n",
    "                in_degrees[pointer] -= 1\n",
    "                if in_degrees[pointer] == 0:\n",
    "                    to_visit.append(pointer)\n",
    "            searched.append(node)\n",
    "        return searched\n",
    "        \n",
    "    def add(self, node, to=None):\n",
    "        if node not in self.graph:\n",
    "            self.graph[node] = []\n",
    "        if to:\n",
    "            if to not in self.graph:\n",
    "                self.graph[to] = []\n",
    "            self.graph[node].append(to)\n",
    "            \n",
    "        if len(self.sort()) != len(self.graph):\n",
    "            raise Exception\n",
    "            \n",
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = DAG()\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        def inner(function):\n",
    "            self.tasks.add(function)\n",
    "            if depends_on:\n",
    "                self.tasks.add(depends_on, function)\n",
    "            return function\n",
    "        return inner\n",
    "    \n",
    "    def run(self):\n",
    "        scheduled = self.tasks.sort()\n",
    "        completed = {}\n",
    "        for task in scheduled:\n",
    "            for node, values in self.tasks.graph.items():\n",
    "                if task in values:\n",
    "                    completed[task] = task(completed[node])\n",
    "            if task not in completed:\n",
    "                completed[task] = task()\n",
    "        return completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the JSON Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll load the JSON file into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the JSON file into a dict object\n",
    "import json\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json') as file:\n",
    "        data_dict = json.load(file)\n",
    "        return data_dict['stories']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the Stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start working with the data by filtering the list of stories to get the most popular stories of the year.\n",
    "\n",
    "We can filter for popular stories by ensuring they are links (not other kinds of posts), have a good number of points, and have some comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(data):\n",
    "    for story in data:\n",
    "        if story['points'] > 50 and story['num_comments'] > 1 and (not story['title'].startswith('Ask HN')):\n",
    "            yield story  \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By converting the dictionary to a CSV file, we have consistent data format for later summarizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import itertools\n",
    "import datetime as dt\n",
    "def build_csv(lines, header=None, file=None):\n",
    "    if header:\n",
    "        lines = itertools.chain([header], lines)\n",
    "    writer = csv.writer(file, delimiter = ',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(dict_stories):\n",
    "    lines = []\n",
    "    for story in dict_stories:\n",
    "        lines.append((story['objectID'], dt.datetime.strptime(story['created_at'], '%Y-%m-%dT%H:%M:%SZ'), story['url'], story['points'], story['title']))\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Title Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the titles of each popular post then we can run the next word frequency task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    read_file = list(csv.reader(csv_file))\n",
    "    for row in read_file[1:]:\n",
    "        yield row[-1]\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the titles, we need to lower case the titles, and to remove the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    for title in titles:\n",
    "        clean_title = title\n",
    "        for char in string.punctuation:\n",
    "            clean_title = clean_title.replace('char', '')\n",
    "        yield clean_title.lower()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Word Frequency Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our word frequency dictionary will have a word as key and the number of times it appears in a text as value. \n",
    "\n",
    "Also, the dictionary won't include words that occur frequently in language, called stop words. For instance, 'the' and 'of' words. To solve this, we will import a module called stop_words that is a tuple of the most commonly used stop words in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import stop_words\n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keyword_dictionary(titles):\n",
    "    freq_words = {}\n",
    "    for title in titles:\n",
    "        for word in title.split():\n",
    "            if word and word not in stop_words:\n",
    "                if word not in freq_words:\n",
    "                    freq_words[word] = 0\n",
    "                else:\n",
    "                    freq_words[word] += 1\n",
    "    return freq_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def order_words(dictionary):\n",
    "    order_dictionary = sorted(dictionary.items(), reverse=True, key=lambda x: x[1])\n",
    "    return order_dictionary[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Running the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can test our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hn:', 192)\n",
      "('new', 183)\n",
      "('google', 150)\n",
      "('bitcoin', 93)\n",
      "('open', 90)\n",
      "('programming', 87)\n",
      "('web', 87)\n",
      "('data', 81)\n",
      "('python', 71)\n",
      "('released', 68)\n",
      "('using', 68)\n",
      "('facebook', 64)\n",
      "('code', 62)\n",
      "('javascript', 61)\n",
      "('game', 59)\n",
      "('[video]', 59)\n",
      "('source', 59)\n",
      "('internet', 58)\n",
      "('free', 56)\n",
      "('app', 55)\n",
      "('microsoft', 54)\n",
      "('linux', 53)\n",
      "('[pdf]', 52)\n",
      "('language', 50)\n",
      "('software', 50)\n",
      "('use', 49)\n",
      "('(2013)', 47)\n",
      "('security', 47)\n",
      "('apple', 46)\n",
      "('time', 46)\n",
      "('startup', 46)\n",
      "('make', 46)\n",
      "('2014', 44)\n",
      "('work', 42)\n",
      "('github', 41)\n",
      "('computer', 39)\n",
      "('heartbleed', 39)\n",
      "('world', 37)\n",
      "('windows', 37)\n",
      "('nsa', 37)\n",
      "('like', 37)\n",
      "('way', 37)\n",
      "('project', 36)\n",
      "('ios', 36)\n",
      "('u.s.', 34)\n",
      "('developer', 33)\n",
      "(\"don't\", 33)\n",
      "('online', 33)\n",
      "('life', 33)\n",
      "('git', 32)\n",
      "('users', 32)\n",
      "('os', 32)\n",
      "('twitter', 32)\n",
      "('big', 32)\n",
      "('guide', 31)\n",
      "('ceo', 31)\n",
      "('mt.', 31)\n",
      "('day', 31)\n",
      "('android', 30)\n",
      "('server', 30)\n",
      "('learning', 30)\n",
      "('design', 30)\n",
      "('api', 30)\n",
      "('says', 30)\n",
      "('browser', 30)\n",
      "('introducing', 29)\n",
      "('firefox', 29)\n",
      "('apps', 29)\n",
      "('built', 28)\n",
      "('mozilla', 28)\n",
      "('engine', 28)\n",
      "('simple', 28)\n",
      "('support', 27)\n",
      "('stop', 27)\n",
      "('does', 27)\n",
      "('tech', 27)\n",
      "('amazon', 27)\n",
      "('gox', 27)\n",
      "('court', 27)\n",
      "('problem', 27)\n",
      "('just', 26)\n",
      "('years', 26)\n",
      "('did', 26)\n",
      "('vs.', 26)\n",
      "('(yc', 26)\n",
      "('million', 26)\n",
      "(\"it's\", 26)\n",
      "('inside', 26)\n",
      "('year', 26)\n",
      "('site', 26)\n",
      "('people', 26)\n",
      "('text', 26)\n",
      "('billion', 25)\n",
      "('library', 25)\n",
      "('website', 25)\n",
      "('c++', 25)\n",
      "('better', 25)\n",
      "('best', 25)\n",
      "('hacker', 24)\n",
      "('development', 24)\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.run()\n",
    "for word in result[order_words]:\n",
    "    print(word)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
